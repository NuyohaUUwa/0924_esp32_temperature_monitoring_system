# è½»é‡çº§ä¼˜åŒ–ç‰ˆFlaskæœåŠ¡å™¨
# æ–‡ä»¶å: lightweight_server.py

import os
import logging
import threading
import time
from datetime import datetime, timedelta, timezone
import json

import psycopg2
from flask import Flask, request, jsonify
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# å®šä¹‰åŒ—äº¬æ—¶åŒºï¼ˆUTC+8ï¼‰
BEIJING_TZ = timezone(timedelta(hours=8))

# é…ç½®
PG_URI = os.getenv("PG_URI")
API_KEY = os.getenv("API_KEY")
PORT = int(os.getenv("PORT", "5000"))

# åˆ›å»ºFlaskåº”ç”¨
app = Flask(__name__)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('server.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ç¦ç”¨Flaskçš„HTTPè¯·æ±‚æ—¥å¿—
logging.getLogger('werkzeug').setLevel(logging.WARNING)

# æ•°æ®åº“è¿æ¥æ± ï¼ˆè½»é‡çº§ï¼‰
class SimpleConnectionPool:
    def __init__(self, uri, min_conn=2, max_conn=5):
        self.uri = uri
        self.min_conn = min_conn
        self.max_conn = max_conn
        self.pool = []
        self.lock = threading.Lock()
        self.stats = {
            'active_connections': 0,
            'connection_errors': 0
        }
        
        # åˆå§‹åŒ–è¿æ¥æ± 
        logger.info(f"åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ±  - è¿æ¥æ•°: {min_conn}")
        for i in range(min_conn):
            try:
                conn = psycopg2.connect(uri)
                self.pool.append(conn)
            except Exception as e:
                logger.error(f"è¿æ¥æ± åˆå§‹åŒ–å¤±è´¥: {e}")
                self.stats['connection_errors'] += 1
        
        logger.info(f"è¿æ¥æ± åˆå§‹åŒ–å®Œæˆ - å¯ç”¨è¿æ¥: {len(self.pool)}")
    
    def get_connection(self):
        with self.lock:
            if self.pool:
                conn = self.pool.pop()
                self.stats['active_connections'] += 1
                return conn
            else:
                try:
                    conn = psycopg2.connect(self.uri)
                    self.stats['active_connections'] += 1
                    return conn
                except Exception as e:
                    self.stats['connection_errors'] += 1
                    logger.error(f"åˆ›å»ºæ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
                    raise
    
    def return_connection(self, conn):
        with self.lock:
            try:
                if len(self.pool) < self.max_conn:
                    self.pool.append(conn)
                else:
                    conn.close()
                self.stats['active_connections'] -= 1
            except Exception as e:
                logger.error(f"å½’è¿˜è¿æ¥å¤±è´¥: {e}")
                self.stats['connection_errors'] += 1
    
    def get_stats(self):
        """è·å–è¿æ¥æ± ç»Ÿè®¡ä¿¡æ¯"""
        with self.lock:
            return {
                'pool_size': len(self.pool),
                'active_connections': self.stats['active_connections'],
                'connection_errors': self.stats['connection_errors']
            }
    
    def health_check(self):
        """æ£€æŸ¥è¿æ¥æ± å¥åº·çŠ¶æ€"""
        try:
            with self.lock:
                if not self.pool:
                    return {'status': 'warning', 'message': 'è¿æ¥æ± ä¸ºç©º'}
                
                # æµ‹è¯•ç¬¬ä¸€ä¸ªè¿æ¥
                test_conn = self.pool[0]
                with test_conn.cursor() as cur:
                    cur.execute("SELECT 1")
                    cur.fetchone()
                
                return {
                    'status': 'healthy',
                    'pool_size': len(self.pool),
                    'active_connections': self.stats['active_connections']
                }
        except Exception as e:
            return {'status': 'error', 'message': str(e)}

# ç®€åŒ–çš„å†…å­˜ç¼“å­˜
class MemoryCache:
    def __init__(self, max_size=100):
        self.cache = {}
        self.max_size = max_size
        self.lock = threading.Lock()
    
    def get(self, key):
        with self.lock:
            data = self.cache.get(key)
            if data and data['expires'] > time.time():
                return data['value']
            return None
    
    def set(self, key, value, ttl=300):
        with self.lock:
            if len(self.cache) >= self.max_size:
                # åˆ é™¤æœ€æ—§çš„æ¡ç›®
                oldest_key = next(iter(self.cache))
                del self.cache[oldest_key]
            
            self.cache[key] = {
                'value': value,
                'expires': time.time() + ttl
            }
    
    def cleanup_expired(self):
        with self.lock:
            current_time = time.time()
            expired_keys = [
                key for key, data in self.cache.items()
                if data['expires'] < current_time
            ]
            for key in expired_keys:
                del self.cache[key]

# ç®€åŒ–çš„æ€§èƒ½ç›‘æ§
class DatabasePerformanceMonitor:
    def __init__(self):
        self.success_count = 0
        self.error_count = 0
        self.lock = threading.Lock()
    
    def record_operation(self, operation_type, duration, success=True):
        """è®°å½•æ•°æ®åº“æ“ä½œæ€§èƒ½"""
        with self.lock:
            if success:
                self.success_count += 1
            else:
                self.error_count += 1
    
    def get_performance_stats(self):
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        with self.lock:
            total = self.success_count + self.error_count
            return {
                'total_operations': total,
                'successful_operations': self.success_count,
                'failed_operations': self.error_count,
                'success_rate': (self.success_count / total * 100) if total > 0 else 0
            }

# ç®€åŒ–çš„å†…å­˜ç¼“å­˜ï¼ˆä»…ç”¨äºä¸´æ—¶æ•°æ®ï¼‰
class SimpleMemoryCache:
    def __init__(self, max_size=100):
        self.cache = {}
        self.max_size = max_size
        self.lock = threading.Lock()
    
    def get(self, key):
        with self.lock:
            data = self.cache.get(key)
            if data and data['expires'] > time.time():
                return data['value']
            return None
    
    def set(self, key, value, ttl=300):
        with self.lock:
            if len(self.cache) >= self.max_size:
                # åˆ é™¤æœ€æ—§çš„æ¡ç›®
                oldest_key = next(iter(self.cache))
                del self.cache[oldest_key]
            
            self.cache[key] = {
                'value': value,
                'expires': time.time() + ttl
            }
    
    def cleanup_expired(self):
        with self.lock:
            current_time = time.time()
            expired_keys = [
                key for key, data in self.cache.items()
                if data['expires'] < current_time
            ]
            for key in expired_keys:
                del self.cache[key]

# å…¨å±€å¯¹è±¡
db_pool = SimpleConnectionPool(PG_URI)
memory_cache = SimpleMemoryCache()
performance_monitor = DatabasePerformanceMonitor()

# åå°ä»»åŠ¡
def background_tasks():
    """åå°æ¸…ç†ä»»åŠ¡"""
    while True:
        try:
            # æ¸…ç†è¿‡æœŸç¼“å­˜
            memory_cache.cleanup_expired()
            
            time.sleep(60)  # æ¯åˆ†é’Ÿæ‰§è¡Œä¸€æ¬¡
        except Exception as e:
            logger.error(f"Background task error: {e}")
            time.sleep(60)

# å¯åŠ¨åå°ä»»åŠ¡
background_thread = threading.Thread(target=background_tasks, daemon=True)
background_thread.start()

# APIè·¯ç”±
@app.route("/health")
def health():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    # æ£€æŸ¥æ•°æ®åº“è¿æ¥æ± çŠ¶æ€
    db_pool_health = db_pool.health_check()
    db_pool_stats = db_pool.get_stats()
    
    return jsonify({
        "status": "ok",
        "time": datetime.now(BEIJING_TZ).isoformat(),
        "database": {
            "connection_pool": db_pool_health,
            "stats": db_pool_stats
        }
    })

@app.route("/api/telemetry", methods=["POST"])
def telemetry():
    """é¥æµ‹æ•°æ®æ¥æ”¶æ¥å£"""
    # API KeyéªŒè¯
    if request.headers.get("X-API-Key") != API_KEY:
        return jsonify({"error": "unauthorized"}), 401
    
    # æ•°æ®éªŒè¯
    try:
        data = request.get_json()
        if not data:
            return jsonify({"error": "invalid JSON"}), 400
        
        required_fields = ["deviceId", "fwVersion", "ip", "uptimeSec", "tempC"]
        missing_fields = [field for field in required_fields if field not in data]
        if missing_fields:
            return jsonify({"error": "missing fields", "fields": missing_fields}), 400
        
        device_id = data["deviceId"]
        fw_version = data["fwVersion"]
        ip = data["ip"]
        uptime_sec = int(data["uptimeSec"])
        temp_c = float(data["tempC"]) if data["tempC"] is not None else None
        
        # æ•°æ®éªŒè¯
        if temp_c is not None and (temp_c < -50 or temp_c > 100):
            return jsonify({"error": "invalid temperature"}), 400
        
    except (ValueError, TypeError) as e:
        logger.error(f"Data validation error: {e}")
        return jsonify({"error": "invalid data format"}), 400
    
    # æ•°æ®åº“æ“ä½œ
    conn = None
    try:
        # è·å–æ•°æ®åº“è¿æ¥
        conn = db_pool.get_connection()
        
        with conn.cursor() as cur:
            # æ‰§è¡Œæ’å…¥æ“ä½œ
            cur.execute("""
                INSERT INTO telemetry (device_id, fw_version, ip, uptime_sec, temp_c)
                VALUES (%s, %s, %s, %s, %s)
            """, (device_id, fw_version, ip, uptime_sec, temp_c))
            
            # è·å–æ’å…¥çš„è®°å½•ID
            cur.execute("SELECT lastval()")
            record_id = cur.fetchone()[0]
        
        # æäº¤äº‹åŠ¡
        conn.commit()
        
        # æ‰“å°å’Œè®°å½•æ¸©åº¦ä¿¡æ¯
        if temp_c is not None:
            temp_info = f"è®¾å¤‡ {device_id} æ¸©åº¦: {temp_c}Â°C (IP: {ip}, è¿è¡Œæ—¶é—´: {uptime_sec}ç§’)"
            print(temp_info)  # æ§åˆ¶å°æ‰“å°
            logger.info(temp_info)  # å†™å…¥æ—¥å¿—
        else:
            device_info = f"è®¾å¤‡ {device_id} æ•°æ®ä¸Šä¼ æˆåŠŸ (IP: {ip}, è¿è¡Œæ—¶é—´: {uptime_sec}ç§’, æ— æ¸©åº¦æ•°æ®)"
            print(device_info)
            logger.info(device_info)
        
        # è®°å½•æ€§èƒ½ç›‘æ§æ•°æ®
        performance_monitor.record_operation("telemetry_insert", 0, True)
        
        return jsonify({
            "ok": True, 
            "timestamp": datetime.now(BEIJING_TZ).isoformat(),
            "record_id": record_id
        })
        
    except Exception as e:
        logger.error(f"æ•°æ®åº“æ“ä½œå¤±è´¥ - è®¾å¤‡: {device_id}, é”™è¯¯: {str(e)}")
        
        # è®°å½•å¤±è´¥çš„æ€§èƒ½ç›‘æ§æ•°æ®
        performance_monitor.record_operation("telemetry_insert", 0, False)
        
        # å›æ»šäº‹åŠ¡ï¼ˆå¦‚æœæœ‰è¿æ¥ï¼‰
        if conn:
            try:
                conn.rollback()
            except Exception as rollback_error:
                logger.error(f"äº‹åŠ¡å›æ»šå¤±è´¥: {rollback_error}")
        
        return jsonify({
            "ok": False, 
            "error": "database error"
        }), 500
    finally:
        if conn:
            try:
                db_pool.return_connection(conn)
            except Exception as return_error:
                logger.error(f"å½’è¿˜è¿æ¥å¤±è´¥: {return_error}")


@app.route("/api/database/status")
def get_database_status():
    """è·å–æ•°æ®åº“çŠ¶æ€å’Œç»Ÿè®¡ä¿¡æ¯"""
    try:
        # è·å–è¿æ¥æ± çŠ¶æ€
        pool_health = db_pool.health_check()
        pool_stats = db_pool.get_stats()
        
        # è·å–æ€§èƒ½ç›‘æ§ç»Ÿè®¡
        performance_stats = performance_monitor.get_performance_stats()
        
        return jsonify({
            "timestamp": datetime.now(BEIJING_TZ).isoformat(),
            "connection_pool": {
                "health": pool_health,
                "statistics": pool_stats
            },
            "performance": performance_stats
        })
        
    except Exception as e:
        logger.error(f"è·å–æ•°æ®åº“çŠ¶æ€å¤±è´¥: {e}")
        return jsonify({"error": "internal error"}), 500

# é”™è¯¯å¤„ç†
@app.errorhandler(404)
def not_found(error):
    return jsonify({"error": "not found"}), 404

@app.errorhandler(500)
def internal_error(error):
    logger.error(f"å†…éƒ¨æœåŠ¡å™¨é”™è¯¯: {error}")
    return jsonify({"error": "internal server error"}), 500

@app.errorhandler(psycopg2.Error)
def database_error(error):
    """æ•°æ®åº“é”™è¯¯å¤„ç†"""
    logger.error(f"æ•°æ®åº“é”™è¯¯: {error}")
    performance_monitor.record_operation("database_error", 0, False)
    return jsonify({"error": "database error"}), 500

def startup_self_check():
    """æœåŠ¡å¯åŠ¨è‡ªæ£€"""
    logger.info("=" * 50)
    logger.info("å¼€å§‹æœåŠ¡å¯åŠ¨è‡ªæ£€...")
    
    # 1. æ£€æŸ¥ç¯å¢ƒå˜é‡
    logger.info("1. æ£€æŸ¥ç¯å¢ƒå˜é‡...")
    if not PG_URI:
        logger.error("âŒ ç¯å¢ƒå˜é‡ PG_URI æœªè®¾ç½®")
        return False
    else:
        logger.info("âœ… ç¯å¢ƒå˜é‡ PG_URI å·²è®¾ç½®")
    
    if not API_KEY:
        logger.error("âŒ ç¯å¢ƒå˜é‡ API_KEY æœªè®¾ç½®")
        return False
    else:
        logger.info("âœ… ç¯å¢ƒå˜é‡ API_KEY å·²è®¾ç½®")
    
    logger.info(f"âœ… æœåŠ¡ç«¯å£: {PORT}")
    
    # 2. æ£€æŸ¥æ•°æ®åº“è¿æ¥
    logger.info("2. æ£€æŸ¥æ•°æ®åº“è¿æ¥...")
    try:
        conn = db_pool.get_connection()
        with conn.cursor() as cur:
            cur.execute("SELECT 1")
            result = cur.fetchone()
            if result:
                logger.info("âœ… æ•°æ®åº“è¿æ¥æ­£å¸¸")
            else:
                logger.error("âŒ æ•°æ®åº“è¿æ¥æµ‹è¯•å¤±è´¥")
                return False
        db_pool.return_connection(conn)
    except Exception as e:
        logger.error(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
        return False
    
    # 3. æ£€æŸ¥è¿æ¥æ± çŠ¶æ€
    logger.info("3. æ£€æŸ¥è¿æ¥æ± çŠ¶æ€...")
    pool_stats = db_pool.get_stats()
    logger.info(f"âœ… è¿æ¥æ± çŠ¶æ€ - å¯ç”¨è¿æ¥: {pool_stats['pool_size']}, æ´»è·ƒè¿æ¥: {pool_stats['active_connections']}")
    
    # 4. æ£€æŸ¥å†…å­˜ç¼“å­˜
    logger.info("4. æ£€æŸ¥å†…å­˜ç¼“å­˜...")
    try:
        memory_cache.set("test_key", "test_value", ttl=1)
        test_value = memory_cache.get("test_key")
        if test_value == "test_value":
            logger.info("âœ… å†…å­˜ç¼“å­˜åŠŸèƒ½æ­£å¸¸")
        else:
            logger.error("âŒ å†…å­˜ç¼“å­˜åŠŸèƒ½å¼‚å¸¸")
            return False
    except Exception as e:
        logger.error(f"âŒ å†…å­˜ç¼“å­˜æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    # 5. æ£€æŸ¥æ€§èƒ½ç›‘æ§
    logger.info("5. æ£€æŸ¥æ€§èƒ½ç›‘æ§...")
    try:
        performance_monitor.record_operation("test_operation", 0, True)
        perf_stats = performance_monitor.get_performance_stats()
        if perf_stats['total_operations'] >= 1:
            logger.info("âœ… æ€§èƒ½ç›‘æ§åŠŸèƒ½æ­£å¸¸")
        else:
            logger.error("âŒ æ€§èƒ½ç›‘æ§åŠŸèƒ½å¼‚å¸¸")
            return False
    except Exception as e:
        logger.error(f"âŒ æ€§èƒ½ç›‘æ§æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    logger.info("âœ… æ‰€æœ‰è‡ªæ£€é¡¹ç›®é€šè¿‡ï¼ŒæœåŠ¡å‡†å¤‡å°±ç»ªï¼")
    logger.info("=" * 50)
    return True

def startup_with_retry(max_retries=3, retry_delay=5):
    """å¸¦é‡è¯•æœºåˆ¶çš„æœåŠ¡å¯åŠ¨è‡ªæ£€"""
    for attempt in range(1, max_retries + 1):
        logger.info(f"ğŸ”„ ç¬¬ {attempt} æ¬¡è‡ªæ£€å°è¯•...")
        
        if startup_self_check():
            logger.info(f"âœ… ç¬¬ {attempt} æ¬¡è‡ªæ£€æˆåŠŸï¼")
            return True
        else:
            if attempt < max_retries:
                logger.warning(f"âš ï¸ ç¬¬ {attempt} æ¬¡è‡ªæ£€å¤±è´¥ï¼Œ{retry_delay}ç§’åé‡è¯•...")
                time.sleep(retry_delay)
            else:
                logger.error(f"âŒ ç¬¬ {attempt} æ¬¡è‡ªæ£€å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
    
    return False

if __name__ == "__main__":
    logger.info(f"æ­£åœ¨å¯åŠ¨è½»é‡çº§æœåŠ¡å™¨ï¼Œç«¯å£: {PORT}")
    
    # æ‰§è¡Œå¸¦é‡è¯•æœºåˆ¶çš„å¯åŠ¨è‡ªæ£€
    if startup_with_retry(max_retries=3, retry_delay=5):
        logger.info(f"ğŸš€ æœåŠ¡å™¨å¯åŠ¨æˆåŠŸï¼Œç›‘å¬ç«¯å£: {PORT}")
        app.run(host="0.0.0.0", port=PORT, threaded=True, debug=False)
    else:
        logger.error("âŒ å¯åŠ¨è‡ªæ£€å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ŒæœåŠ¡æ— æ³•å¯åŠ¨")
        exit(1)
